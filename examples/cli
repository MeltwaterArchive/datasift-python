#!/bin/bash

set -e

DU=${1:-$DU}
DK=${2:-$DK}

staging() {
  ./bin/datasift --api http://api.stagingdatasift.com -a $DU $DK "$@" | jq .
}

export PYTHONPATH=.

# Core API
staging -c dpu -p stream 42
staging -c usage
staging -c usage -p period hour
staging -c usage -p period day
staging -c balance
staging -c validate -p csdl '{ wrong }'
staging -c validate -p csdl 'source.id == "42"'
staging -c compile -p csdl 'source.id == "42"'
staging -c pull -p subscription_id 42

# Managed Sources
src=$(staging -e sources -c create \
     -p source_type instagram -p name api \
     -p auth '[{"parameters":{"value":"TOKEN"}}]' \
     -p resources '[{"parameters":{"value":"cats","type":"tag"}}]' \
     -p parameters '{"comments":true,"likes":false}' | jq -r .body.id)
staging -e sources -c start -p source_id $src
staging -e sources -c stop -p source_id $src
staging -e sources -c get
staging -e sources -c get -p source_type instagram
staging -e sources -c get -p source_type facebook_page
staging -e sources -c get -p source_type instagram -p page 2
staging -e sources -c get -p source_id $src
staging -e sources -c log -p source_id $src -p page 2 -p per_page 1
staging -e sources -c delete -p source_id $src

# Historics Preview (dummy values, all of these cause errors)
staging -e preview -c create -p stream 42 -p start 124 -p parameters '["target","analysis"]' -p sources '["twitter"]' -p end 1325549800
staging -e preview -c get -p preview_id 42

# Historics (dummy values, all of these cause errors)
staging -e historics -c prepare -p stream 42 -p start 42 -p end 43 -p name test -p sources '["twitter"]' -p sample 0.01
staging -e historics -c start -p historics_id 42
staging -e historics -c update -p name test -p historics_id 42
staging -e historics -c stop -p historics_id 42
staging -e historics -c stop -p historics_id 42 -p reason "just do it"
staging -e historics -c status -p start 42 -p end 42
staging -e historics -c status -p start 42 -p end 42 -p sources '["facebook"]'
staging -e historics -c status -p start 42 -p end 42 -p sources '["twitter"]'
staging -e historics -c update -p historics_id 42 -p name "new name"
staging -e historics -c delete -p historics_id 42
staging -e historics -c get
staging -e historics -c get -p maximum 3 -p with_estimate true

# Push (mostly dummy values)
staging -e push -c validate -p output_type http -p output_params '{"format":"json","delivery_frequency":0,"url":"http://datasift.com"}'
staging -e push -c create -p output_type http -p output_params '{"format":"json","delivery_frequency":0,"url":"http://datasift.com"}' -p from_hash 4242 -p stream_or_id 42 -p name test
staging -e push -c create -p output_type http -p output_params '{"format":"json","delivery_frequency":0,"url":"http://datasift.com"}' -p from_hash 4242 -p stream_or_id 42 -p name test -p initial_status 42
staging -e push -c create -p output_type http -p output_params '{"format":"json","delivery_frequency":0,"url":"http://datasift.com"}' -p from_hash 4242 -p stream_or_id 42 -p name test -p initial_status 42 -p start 84
staging -e push -c create -p output_type http -p output_params '{"format":"json","delivery_frequency":0,"url":"http://datasift.com"}' -p from_hash 4242 -p stream_or_id 42 -p name test -p initial_status 42 -p end 84
staging -e push -c pause -p subscription_id 42
staging -e push -c stop -p subscription_id 42
staging -e push -c delete -p subscription_id 42
staging -e push -c log -p subscription_id 42
staging -e push -c log
staging -e push -c log -p page 2 -p per_page 1
staging -e push -c log -p order_by request_time
staging -e push -c log -p order_dir desc
staging -e push -c log -p order_dir asc
staging -e push -c get -p stream 42
staging -e push -c get -p historics_id 42
staging -e push -c get -p include_finished false
staging -e push -c get -p include_finished true
staging -e push -c create -p name test-api -p from_hash true -p stream_or_id 14d12c0c828fd84509c29dea44591e5e -p output_type pull -p output_params '{"format": "json_new_line"}'

